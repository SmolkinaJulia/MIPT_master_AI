{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LogisticRegression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elDRkDXo6FoT"
      },
      "source": [
        "### Создание датасета"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cT4Uen-Q75Ak"
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = load_breast_cancer(return_X_y=True)\n",
        "partitions = []\n",
        "for i in range(5):\n",
        "    random_state = 47 + 112 * i\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=random_state)\n",
        "    partitions.append((X_train, X_test, y_train, y_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VUGS7PfW6Ony"
      },
      "source": [
        "## Логистическая регрессия на numpy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "os7DIphq-G9n"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class BinaryLogisticRegression:\n",
        "\n",
        "    def __init__(self, eta=1e-4, random_state=176):\n",
        "        self.eta = eta\n",
        "        np.random.seed(random_state)\n",
        "    \n",
        "    def fit_epoch(self, X, y):\n",
        "        # добавляем нулевой признак константу -1\n",
        "        X = np.concatenate([-np.ones_like(X[:,:1]), X], axis=1)\n",
        "        if not hasattr(self, \"weight_\"):\n",
        "            self.weight_ = np.zeros_like(X[0], dtype=float)\n",
        "        order = np.arange(len(X))\n",
        "        np.random.shuffle(order)\n",
        "        X, y = X[order], y[order]\n",
        "        for elem, label in zip(X, y):\n",
        "            # вероятность положительного класса\n",
        "            score = 1 - 1 / (1 + np.exp(np.dot(self.weight_, elem)))\n",
        "            # w' <- w + eta (y - p) * x\n",
        "            self.weight_ += self.eta * ((label - score) * elem)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        return (self._score(X) >= 0).astype(\"int\")\n",
        "    \n",
        "    def _score(self, X):\n",
        "        return np.dot(X, self.weight_[1:]) - self.weight_[0]\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        return 1 - 1 / (1 + np.exp(self._score(X)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I8Op--iHBFLe",
        "outputId": "fa43f54d-3f85-41d5-b3e6-4f61a454820f"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "scores = []\n",
        "for X_train, X_test, y_train, y_test in partitions:\n",
        "    cls = BinaryLogisticRegression(eta=1e-3)\n",
        "    best_train_score, best_weight = 0.0, None\n",
        "    patience = 0\n",
        "    for i in range(100):\n",
        "        cls.fit_epoch(X_train, y_train)\n",
        "        y_train_pred = cls.predict(X_train)\n",
        "        train_score = accuracy_score(y_train, y_train_pred)\n",
        "        y_test_pred = cls.predict(X_test)\n",
        "        test_score = accuracy_score(y_test, y_test_pred)\n",
        "        if train_score > best_train_score:\n",
        "            best_train_score, best_weight = train_score, np.copy(cls.weight_)\n",
        "            patience = 0\n",
        "        else:\n",
        "            patience += 1\n",
        "            if patience >= 10:\n",
        "                # print(\"Ran out of patience after {} epochs\".format(i+1))\n",
        "                cls.weight_ = best_weight\n",
        "                break\n",
        "    y_test_pred = cls.predict(X_test)\n",
        "    test_score = accuracy_score(y_test, y_test_pred)\n",
        "    # print(\"Final test score {:.2f}\".format(100 * test_score))\n",
        "    scores.append(test_score)\n",
        "print(*(\"{:.2f}\".format(100*x) for x in scores))\n",
        "print(\"{:.2f}\".format(100*np.mean(scores)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: RuntimeWarning: overflow encountered in exp\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "93.01 92.31 90.91 93.01 93.01\n",
            "92.45\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW9_bEvXXQfy"
      },
      "source": [
        "## Логистическая регрессия из sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXLnL148D9LU",
        "outputId": "1746a9f1-4587-4d1b-c33d-35cea1a0853d"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
        "\n",
        "sklearn_scores = []\n",
        "for X_train, X_test, y_train, y_test in partitions:\n",
        "    cls = LogisticRegression()\n",
        "    cls.fit(X_train, y_train)\n",
        "    y_train_pred = cls.predict(X_train)\n",
        "    train_score = accuracy_score(y_train, y_train_pred)\n",
        "    y_test_pred = cls.predict(X_test)\n",
        "    test_score = accuracy_score(y_test, y_test_pred)\n",
        "    # print(\"{:.2f} {:.2f}\".format(100*train_score, 100*test_score))\n",
        "    sklearn_scores.append(test_score)\n",
        "print(*(\"{:.2f}\".format(100*x) for x in sklearn_scores))\n",
        "print(\"{:.2f}\".format(100*np.mean(sklearn_scores)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "92.31 95.80 95.80 98.60 97.90\n",
            "96.08\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWgdAApOXUTo"
      },
      "source": [
        "## Логистическая регрессия на pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pvwyMKWB4bR"
      },
      "source": [
        "import torch\n",
        "\n",
        "class PytorchLogisticRegression(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, n_features, lr=1e-3):\n",
        "        super(PytorchLogisticRegression, self).__init__()\n",
        "        self.linear = torch.nn.Linear(n_features, 1)\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "        self.criterion = torch.nn.BCELoss()\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
        "        torch.nn.init.zeros_(self.linear.weight)\n",
        "\n",
        "    def forward(self, X):\n",
        "        logits = self.linear(X)\n",
        "        probs = self.sigmoid(logits)\n",
        "        return probs[:,0]\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        with torch.no_grad():\n",
        "            y = self(X)\n",
        "        return y.numpy()\n",
        "\n",
        "    def predict(self, X):\n",
        "        probs = self.predict_proba(X)\n",
        "        return (probs >= 0.5).astype(\"int\")\n",
        "\n",
        "    def fit_epoch(self, X, y):\n",
        "        order = np.arange(len(X))\n",
        "        np.random.shuffle(order)\n",
        "        X, y = X[order], y[order]\n",
        "        for r, (elem, label) in enumerate(zip(X, y)):\n",
        "            label = torch.unsqueeze(label, dim=0)\n",
        "            self.optimizer.zero_grad()\n",
        "            prob = self.forward(elem[None,:]) # n -> (1 \\times n)\n",
        "            loss = self.criterion(prob, label)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "        return self"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imettTkO-Us7",
        "outputId": "b27da967-99bb-41cc-9feb-d57b2010170c"
      },
      "source": [
        "from torch import Tensor, LongTensor\n",
        "import copy\n",
        "\n",
        "scores = []\n",
        "for X_train, X_test, y_train, y_test in partitions:\n",
        "    X_train, X_test = Tensor(X_train), Tensor(X_test)\n",
        "    y_train, y_test = Tensor(y_train), Tensor(y_test)\n",
        "    cls = PytorchLogisticRegression(n_features=X_train.shape[1], lr=1e-3)\n",
        "    best_train_score, best_weights = 0.0, None\n",
        "    patience = 0\n",
        "    for i in range(100):\n",
        "        cls.fit_epoch(X_train, y_train)\n",
        "        y_train_pred = cls.predict(X_train)\n",
        "        train_score = accuracy_score(y_train, y_train_pred)\n",
        "        y_test_pred = cls.predict(X_test)\n",
        "        test_score = accuracy_score(y_test, y_test_pred)\n",
        "        if train_score > best_train_score:\n",
        "            best_train_score = train_score\n",
        "            best_weights = copy.deepcopy(cls.state_dict())\n",
        "            patience = 0\n",
        "        else:\n",
        "            patience += 1\n",
        "            if patience >= 10:\n",
        "                # print(\"Ran out of patience after {} epochs\".format(i+1))\n",
        "                break\n",
        "    if best_weights is not None:\n",
        "        cls.load_state_dict(best_weights)\n",
        "    y_test_pred = cls.predict(X_test)\n",
        "    test_score = accuracy_score(y_test, y_test_pred)\n",
        "    # print(\"Final test score {:.2f}\".format(100 * test_score))\n",
        "    scores.append(test_score)\n",
        "print(*(\"{:.2f}\".format(100*x) for x in scores))\n",
        "print(\"{:.2f}\".format(100*np.mean(scores)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "92.31 91.61 93.71 95.10 96.50\n",
            "93.85\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmHrQL9MHvu_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}